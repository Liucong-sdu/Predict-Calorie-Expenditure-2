{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4976966b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "df1=pd.read_csv('train.csv')\n",
    "df2 = pd.read_csv('test.csv')\n",
    "\n",
    "# 独热编码（处理未知类别）\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "encoded_train = encoder.fit_transform(df1[['Sex']])\n",
    "encoded_test = encoder.transform(df2[['Sex']])\n",
    "\n",
    "# 合并编码结果\n",
    "encoded_df_train = pd.DataFrame(encoded_train, columns=encoder.get_feature_names_out(['Sex']))\n",
    "encoded_df_test = pd.DataFrame(encoded_test, columns=encoder.get_feature_names_out(['Sex']))\n",
    "df1 = pd.concat([df1, encoded_df_train], axis=1).drop('Sex', axis=1)\n",
    "df2 = pd.concat([df2, encoded_df_test], axis=1).drop('Sex', axis=1)\n",
    "\n",
    "# 合成BMI和体脂率\n",
    "def BMI(df):\n",
    "    df['BMI'] = df['Weight'] / ((df['Height']/100 )**2)\n",
    "    return df\n",
    "\n",
    "def Body_Fat_Percentage(df):\n",
    "    df['Body_Fat_Percentage'] = 1.20 * df['BMI'] + 0.23 * df['Age'] - 5.4 - 10.8*df['Sex_male']\n",
    "    return df\n",
    "df1 = BMI(df1)\n",
    "df2 = BMI(df2)\n",
    "df1 = Body_Fat_Percentage(df1)\n",
    "df2 = Body_Fat_Percentage(df2)\n",
    "\n",
    "df1.to_csv('train_processed.csv', index=False)\n",
    "df2.to_csv('test_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7b98ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from xgboost import XGBRegressor as XGBR\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "df1 = pd.read_csv('train_processed.csv')\n",
    "\n",
    "X, y =df1.drop(['Calories'], axis=1).values,df1['Calories'].values\n",
    "\n",
    "# 创建模型（示例用逻辑回归）\n",
    "model = XGBR(objective=\"reg:squarederror\",\n",
    "                n_estimators=1000,\n",
    "                learning_rate=0.01,\n",
    "                max_depth=6,\n",
    "                random_state=42,\n",
    "                \n",
    "                n_jobs=4,\n",
    "                tree_method=\"gpu_hist\",\n",
    "                predictor=\"gpu_predictor\",\n",
    "                gpu_id=0,\n",
    "                eval_metric=\"mae\",\n",
    "                           )\n",
    "\n",
    "# 创建 KFold 交叉验证对象（假设 K=5）\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# 存储每折的准确率\n",
    "scores = []\n",
    "\n",
    "# 遍历每一折\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # 训练模型\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # 预测并计算准确率\n",
    "    y_pred = model.predict(X_test)\n",
    "    scores.append(r2_score(y_test, y_pred))  # 使用 R² 分数\n",
    "# 或\n",
    "\n",
    "\n",
    "# 输出平均准确率\n",
    "print(\"交叉验证平均准确率:\", sum(scores)/len(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39809b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "获取的目标列是： Calories\n",
      "数据分隔完成（含验证集）\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5623]\tvalid_0's l2: 12.8858\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[6423]\tvalid_0's l2: 13.2815\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5659]\tvalid_0's l2: 12.8932\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[6753]\tvalid_0's l2: 12.5304\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5745]\tvalid_0's l2: 12.999\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[9999]\tvalid_0's l2: 12.9573\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[9095]\tvalid_0's l2: 13.3378\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[7979]\tvalid_0's l2: 12.9796\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[7560]\tvalid_0's l2: 12.6834\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[8999]\tvalid_0's l2: 13.0309\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2541]\tvalid_0's l2: 12.9562\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2751]\tvalid_0's l2: 13.3851\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2457]\tvalid_0's l2: 12.9807\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2781]\tvalid_0's l2: 12.6068\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2518]\tvalid_0's l2: 13.0945\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2416]\tvalid_0's l2: 13.0745\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3080]\tvalid_0's l2: 13.2906\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2245]\tvalid_0's l2: 13.0532\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3096]\tvalid_0's l2: 12.6029\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2461]\tvalid_0's l2: 13.2075\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5037]\tvalid_0's l2: 13.1025\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[6862]\tvalid_0's l2: 13.3151\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4909]\tvalid_0's l2: 13.0854\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5270]\tvalid_0's l2: 12.6687\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5489]\tvalid_0's l2: 13.1772\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5554]\tvalid_0's l2: 13.0872\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5690]\tvalid_0's l2: 13.4347\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[7048]\tvalid_0's l2: 13.0017\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[7030]\tvalid_0's l2: 12.6247\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5372]\tvalid_0's l2: 13.204\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[9997]\tvalid_0's l2: 13.2058\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\tvalid_0's l2: 13.6667\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\tvalid_0's l2: 13.2484\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[9990]\tvalid_0's l2: 12.8076\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\tvalid_0's l2: 13.2608\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[6276]\tvalid_0's l2: 13.3102\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[8775]\tvalid_0's l2: 13.5285\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[9998]\tvalid_0's l2: 13.0644\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[8987]\tvalid_0's l2: 12.7003\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[6880]\tvalid_0's l2: 13.2988\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[9496]\tvalid_0's l2: 12.9404\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[9653]\tvalid_0's l2: 13.3883\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[8976]\tvalid_0's l2: 12.9915\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[9259]\tvalid_0's l2: 12.6254\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[7673]\tvalid_0's l2: 13.1505\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[9733]\tvalid_0's l2: 12.9394\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[9894]\tvalid_0's l2: 13.3403\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[9998]\tvalid_0's l2: 12.9402\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[7305]\tvalid_0's l2: 12.6786\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[6655]\tvalid_0's l2: 13.1599\n",
      "LGBM 最佳验证MAE: 2.1656\n",
      "LGBMRegressor(colsample_bytree=0.8, learning_rate=0.008, max_depth=8,\n",
      "              n_estimators=10000, objective='regression', random_state=8,\n",
      "              reg_alpha=0.001, reg_lambda=0.1, verbosity=-1)\n",
      "XGB 最佳验证MAE: 2.1400\n",
      "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=0.9, device=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric='mae', feature_types=None,\n",
      "             feature_weights=None, gamma=0.1, gpu_id=0, grow_policy=None,\n",
      "             importance_type=None, interaction_constraints=None,\n",
      "             learning_rate=0.007, max_bin=None, max_cat_threshold=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=7,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, multi_strategy=None, n_estimators=10000,\n",
      "             n_jobs=4, ...)\n",
      "CatBoost 最佳验证MAE: 2.1177\n",
      "<catboost.core.CatBoostRegressor object at 0x000001A61A6340A0>\n",
      "\n",
      "所有模型训练完成\n",
      "LGBM 预测完成\n",
      "XGB 预测完成\n",
      "CatBoost 预测完成\n",
      "结果已保存至：myoutput\\LGBM_submission.csv\n",
      "结果已保存至：myoutput\\XGB_submission.csv\n",
      "结果已保存至：myoutput\\CatBoost_submission.csv\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import KFold\n",
    "from lightgbm import LGBMRegressor as LGBMR, early_stopping\n",
    "from xgboost import XGBRegressor as XGBR\n",
    "from catboost import CatBoostRegressor as CBR\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split, ParameterSampler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "class cfg:\n",
    "    trainfilepath = \"train_processed.csv\"\n",
    "    testfilepath = \"test_processed.csv\"\n",
    "    outfilepath = \"myoutput\"\n",
    "    state = 8\n",
    "    n_iter = 10  # 每个模型的参数搜索次数\n",
    "    early_stop_rounds = 50\n",
    "\n",
    "class future_engineer:\n",
    "    traindata = pd.read_csv(cfg.trainfilepath)\n",
    "    testdata = pd.read_csv(cfg.testfilepath)\n",
    "    headerstrian = set(traindata.columns)\n",
    "    headerstest = set(testdata.columns)\n",
    "    target = headerstrian.symmetric_difference(headerstest).pop()\n",
    "    print(\"获取的目标列是：\", target)\n",
    "\n",
    "class dataset_split:\n",
    "    # 划分训练集和验证集（保持原始测试集不变）\n",
    "    X_full = future_engineer.traindata.drop([future_engineer.target], axis=1)\n",
    "    Y_full = future_engineer.traindata[future_engineer.target]\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    X_test = future_engineer.testdata\n",
    "    print(\"数据分隔完成（含验证集）\")\n",
    "\n",
    "class HyperparameterSearch:\n",
    "    @staticmethod\n",
    "    def lgbm_search():\n",
    "        param_dist = {\n",
    "            'learning_rate': [0.005, 0.006,0.007,0.009,0.008,0.01, 0.02],\n",
    "            'max_depth': [4,5,6,7,8],\n",
    "            'colsample_bytree': [0.8, 0.9, 0.95],\n",
    "            'reg_alpha': [0.001, 0.01, 0.1],\n",
    "            'reg_lambda': [0.001, 0.01, 0.1]\n",
    "        }\n",
    "        best_score = np.inf\n",
    "        best_model = None\n",
    "        \n",
    "        for params in ParameterSampler(param_dist, n_iter=cfg.n_iter, random_state=cfg.state):\n",
    "            score_list = []\n",
    "            model = LGBMR(\n",
    "                objective=\"regression\",\n",
    "                n_estimators=10000,\n",
    "                **params,\n",
    "                random_state=cfg.state,\n",
    "                verbosity=-1\n",
    "            )\n",
    "            for train_index, val_index in dataset_split.kf.split(dataset_split.X_full):\n",
    "                # 修改前（错误）\n",
    "                \n",
    "                X_train = dataset_split.X_full.iloc[train_index]\n",
    "                X_val = dataset_split.X_full.iloc[val_index]\n",
    "                y_train = dataset_split.Y_full.iloc[train_index]\n",
    "                y_val = dataset_split.Y_full.iloc[val_index]\n",
    "                model.fit(\n",
    "                    X_train, y_train,\n",
    "                    eval_set=[(X_val, y_val)],\n",
    "                    callbacks=[early_stopping(cfg.early_stop_rounds)],)\n",
    "                pred = model.predict(X_val)\n",
    "                score_list.append(mean_absolute_error(y_val, pred))\n",
    "            score = np.mean(score_list)\n",
    "            score_list = []  # 清空列表以便下次使用\n",
    "            if score < best_score:\n",
    "                best_score = score\n",
    "                best_model = model\n",
    "        print(f\"LGBM 最佳验证MAE: {best_score:.4f}\")\n",
    "        print(best_model)\n",
    "        return best_model\n",
    "\n",
    "    @staticmethod\n",
    "    def xgb_search():\n",
    "        param_dist = {\n",
    "            'learning_rate': [0.005, 0.006,0.007,0.009,0.008,0.01, 0.02],\n",
    "            'max_depth': [4, 5,6, 7,8],\n",
    "            'colsample_bytree': [0.8, 0.9, 0.95],\n",
    "            'reg_alpha': [0.001, 0.01, 0.1],\n",
    "            'gamma': [0, 0.1, 0.2]\n",
    "        }\n",
    "        best_score = np.inf\n",
    "        best_model = None\n",
    "\n",
    "        for params in ParameterSampler(param_dist, n_iter=cfg.n_iter, random_state=cfg.state):\n",
    "            score_list = []\n",
    "            model = XGBR(\n",
    "                objective=\"reg:squarederror\",\n",
    "                n_estimators=10000,\n",
    "                \n",
    "                **params,\n",
    "                random_state=cfg.state,\n",
    "                n_jobs=4,\n",
    "                tree_method=\"gpu_hist\",\n",
    "                predictor=\"gpu_predictor\",\n",
    "                gpu_id=0,\n",
    "                eval_metric=\"mae\",\n",
    "            )\n",
    "            for train_index, val_index in dataset_split.kf.split(dataset_split.X_full):\n",
    "                # 修改前（错误）\n",
    "                \n",
    "                X_train = dataset_split.X_full.iloc[train_index]\n",
    "                X_val = dataset_split.X_full.iloc[val_index]\n",
    "                y_train = dataset_split.Y_full.iloc[train_index]\n",
    "                y_val = dataset_split.Y_full.iloc[val_index]\n",
    "                model.fit(\n",
    "                    X_train, y_train,\n",
    "                    eval_set=[(X_val, y_val)],\n",
    "                    verbose=False,)\n",
    "                pred = model.predict(X_val)\n",
    "                score_list.append(mean_absolute_error(y_val, pred))\n",
    "            score = np.mean(score_list)\n",
    "            score_list = []  # 清空列表以便下次使用\n",
    "            if score < best_score:\n",
    "                best_score = score\n",
    "                best_model = model\n",
    "        print(f\"XGB 最佳验证MAE: {best_score:.4f}\")\n",
    "        print(best_model)\n",
    "        return best_model\n",
    "\n",
    "    @staticmethod\n",
    "    def cat_search():\n",
    "        param_dist = {\n",
    "                'learning_rate': [0.005, 0.006,0.007,0.009,0.008,0.01, 0.02],\n",
    "                'depth': [4,5, 6,7, 8],  # 替换max_depth → depth[3,5,7](@ref)\n",
    "                'rsm': [0.8, 0.9, 0.95],  # 替换colsample_bytree → max_features[3,5](@ref)\n",
    "                'l2_leaf_reg': [0.1, 0.5, 1.0, 10]  # 扩展范围[3](@ref)\n",
    "}\n",
    "        best_score = np.inf\n",
    "        best_model = None\n",
    "\n",
    "        for params in ParameterSampler(param_dist, n_iter=cfg.n_iter, random_state=cfg.state):\n",
    "            score_list = []\n",
    "            model = CBR(\n",
    "                loss_function=\"RMSE\",\n",
    "                iterations=10000,\n",
    "                **params,\n",
    "                random_state=cfg.state,\n",
    "                verbose=0,\n",
    "                early_stopping_rounds=cfg.early_stop_rounds\n",
    "            )\n",
    "            for train_index, val_index in dataset_split.kf.split(dataset_split.X_full):\n",
    "                # 修改前（错误）\n",
    "                \n",
    "                X_train = dataset_split.X_full.iloc[train_index]\n",
    "                X_val = dataset_split.X_full.iloc[val_index]\n",
    "                y_train = dataset_split.Y_full.iloc[train_index]\n",
    "                y_val = dataset_split.Y_full.iloc[val_index]\n",
    "                model.fit(\n",
    "                    X_train, y_train,\n",
    "                    eval_set=[(X_val, y_val)],\n",
    "                    verbose=False,)\n",
    "                pred = model.predict(X_val)\n",
    "                score_list.append(mean_absolute_error(y_val, pred))\n",
    "            score = np.mean(score_list)\n",
    "            score_list = []  # 清空列表以便下次使用\n",
    "            if score < best_score:\n",
    "                best_score = score\n",
    "                best_model = model\n",
    "        print(f\"CatBoost 最佳验证MAE: {best_score:.4f}\")\n",
    "        print(best_model)\n",
    "        return best_model\n",
    "\n",
    "class model:\n",
    "    models = {\n",
    "        \"LGBM\": HyperparameterSearch.lgbm_search(),\n",
    "        \"XGB\": HyperparameterSearch.xgb_search(),\n",
    "        \"CatBoost\": HyperparameterSearch.cat_search()\n",
    "    }\n",
    "    print(\"\\n所有模型训练完成\")\n",
    "\n",
    "class predict:\n",
    "    predictions = {}\n",
    "    for name, model in model.models.items():\n",
    "        predictions[name] = model.predict(dataset_split.X_test)\n",
    "        print(f\"{name} 预测完成\")\n",
    "\n",
    "class writefile:\n",
    "    test_ids = future_engineer.testdata[\"id\"]\n",
    "    for name, pred in predict.predictions.items():\n",
    "        filename = os.path.join(cfg.outfilepath, f\"{name}_submission.csv\")\n",
    "        pd.DataFrame({\n",
    "            \"id\": test_ids,\n",
    "            future_engineer.target: pred\n",
    "        }).to_csv(filename, index=False)\n",
    "        print(f\"结果已保存至：{filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JUNzi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
