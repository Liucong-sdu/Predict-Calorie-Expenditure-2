{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4976966b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "df1=pd.read_csv('train.csv')\n",
    "df2 = pd.read_csv('test.csv')\n",
    "\n",
    "# 独热编码（处理未知类别）\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "encoded_train = encoder.fit_transform(df1[['Sex']])\n",
    "encoded_test = encoder.transform(df2[['Sex']])\n",
    "\n",
    "# 合并编码结果\n",
    "encoded_df_train = pd.DataFrame(encoded_train, columns=encoder.get_feature_names_out(['Sex']))\n",
    "encoded_df_test = pd.DataFrame(encoded_test, columns=encoder.get_feature_names_out(['Sex']))\n",
    "df1 = pd.concat([df1, encoded_df_train], axis=1).drop('Sex', axis=1)\n",
    "df2 = pd.concat([df2, encoded_df_test], axis=1).drop('Sex', axis=1)\n",
    "\n",
    "# 合成BMI和体脂率\n",
    "def BMI(df):\n",
    "    df['BMI'] = df['Weight'] / ((df['Height']/100 )**2)\n",
    "    return df\n",
    "\n",
    "def Body_Fat_Percentage(df):\n",
    "    df['Body_Fat_Percentage'] = 1.20 * df['BMI'] + 0.23 * df['Age'] - 5.4 - 10.8*df['Sex_male']\n",
    "    return df\n",
    "df1 = BMI(df1)\n",
    "df2 = BMI(df2)\n",
    "df1 = Body_Fat_Percentage(df1)\n",
    "df2 = Body_Fat_Percentage(df2)\n",
    "\n",
    "df1.to_csv('train_processed.csv', index=False)\n",
    "df2.to_csv('test_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7b98ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from xgboost import XGBRegressor as XGBR\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "df1 = pd.read_csv('train_processed.csv')\n",
    "\n",
    "X, y =df1.drop(['Calories'], axis=1).values,df1['Calories'].values\n",
    "\n",
    "# 创建模型（示例用逻辑回归）\n",
    "model = XGBR(objective=\"reg:squarederror\",\n",
    "                n_estimators=1000,\n",
    "                learning_rate=0.01,\n",
    "                max_depth=6,\n",
    "                random_state=42,\n",
    "                \n",
    "                n_jobs=4,\n",
    "                tree_method=\"gpu_hist\",\n",
    "                predictor=\"gpu_predictor\",\n",
    "                gpu_id=0,\n",
    "                eval_metric=\"mae\",\n",
    "                           )\n",
    "\n",
    "# 创建 KFold 交叉验证对象（假设 K=5）\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# 存储每折的准确率\n",
    "scores = []\n",
    "\n",
    "# 遍历每一折\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # 训练模型\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # 预测并计算准确率\n",
    "    y_pred = model.predict(X_test)\n",
    "    scores.append(r2_score(y_test, y_pred))  # 使用 R² 分数\n",
    "# 或\n",
    "\n",
    "\n",
    "# 输出平均准确率\n",
    "print(\"交叉验证平均准确率:\", sum(scores)/len(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39809b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "获取的目标列是： Calories\n",
      "数据分隔完成（含验证集）\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5623]\tvalid_0's l2: 12.8858\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[6423]\tvalid_0's l2: 13.2815\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5659]\tvalid_0's l2: 12.8932\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[6753]\tvalid_0's l2: 12.5304\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5745]\tvalid_0's l2: 12.999\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[9999]\tvalid_0's l2: 12.9573\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[9095]\tvalid_0's l2: 13.3378\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[7979]\tvalid_0's l2: 12.9796\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[7560]\tvalid_0's l2: 12.6834\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[8999]\tvalid_0's l2: 13.0309\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2541]\tvalid_0's l2: 12.9562\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2751]\tvalid_0's l2: 13.3851\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2457]\tvalid_0's l2: 12.9807\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2781]\tvalid_0's l2: 12.6068\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2518]\tvalid_0's l2: 13.0945\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2416]\tvalid_0's l2: 13.0745\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3080]\tvalid_0's l2: 13.2906\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2245]\tvalid_0's l2: 13.0532\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3096]\tvalid_0's l2: 12.6029\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2461]\tvalid_0's l2: 13.2075\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5037]\tvalid_0's l2: 13.1025\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[6862]\tvalid_0's l2: 13.3151\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4909]\tvalid_0's l2: 13.0854\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5270]\tvalid_0's l2: 12.6687\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5489]\tvalid_0's l2: 13.1772\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5554]\tvalid_0's l2: 13.0872\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5690]\tvalid_0's l2: 13.4347\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[7048]\tvalid_0's l2: 13.0017\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[7030]\tvalid_0's l2: 12.6247\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5372]\tvalid_0's l2: 13.204\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[9997]\tvalid_0's l2: 13.2058\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\tvalid_0's l2: 13.6667\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\tvalid_0's l2: 13.2484\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[9990]\tvalid_0's l2: 12.8076\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\tvalid_0's l2: 13.2608\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[6276]\tvalid_0's l2: 13.3102\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[8775]\tvalid_0's l2: 13.5285\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[9998]\tvalid_0's l2: 13.0644\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[8987]\tvalid_0's l2: 12.7003\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[6880]\tvalid_0's l2: 13.2988\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[9496]\tvalid_0's l2: 12.9404\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[9653]\tvalid_0's l2: 13.3883\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[8976]\tvalid_0's l2: 12.9915\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[9259]\tvalid_0's l2: 12.6254\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[7673]\tvalid_0's l2: 13.1505\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[9733]\tvalid_0's l2: 12.9394\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[9894]\tvalid_0's l2: 13.3403\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[9998]\tvalid_0's l2: 12.9402\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[7305]\tvalid_0's l2: 12.6786\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[6655]\tvalid_0's l2: 13.1599\n",
      "LGBM 最佳验证MAE: 2.1656\n",
      "LGBMRegressor(colsample_bytree=0.8, learning_rate=0.008, max_depth=8,\n",
      "              n_estimators=10000, objective='regression', random_state=8,\n",
      "              reg_alpha=0.001, reg_lambda=0.1, verbosity=-1)\n",
      "XGB 最佳验证MAE: 2.1400\n",
      "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=0.9, device=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric='mae', feature_types=None,\n",
      "             feature_weights=None, gamma=0.1, gpu_id=0, grow_policy=None,\n",
      "             importance_type=None, interaction_constraints=None,\n",
      "             learning_rate=0.007, max_bin=None, max_cat_threshold=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=7,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, multi_strategy=None, n_estimators=10000,\n",
      "             n_jobs=4, ...)\n",
      "CatBoost 最佳验证MAE: 2.1177\n",
      "<catboost.core.CatBoostRegressor object at 0x000001A61A6340A0>\n",
      "\n",
      "所有模型训练完成\n",
      "LGBM 预测完成\n",
      "XGB 预测完成\n",
      "CatBoost 预测完成\n",
      "结果已保存至：myoutput\\LGBM_submission.csv\n",
      "结果已保存至：myoutput\\XGB_submission.csv\n",
      "结果已保存至：myoutput\\CatBoost_submission.csv\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import KFold\n",
    "from lightgbm import LGBMRegressor as LGBMR, early_stopping\n",
    "from xgboost import XGBRegressor as XGBR\n",
    "from catboost import CatBoostRegressor as CBR\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split, ParameterSampler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "class cfg:\n",
    "    trainfilepath = \"train_processed.csv\"\n",
    "    testfilepath = \"test_processed.csv\"\n",
    "    outfilepath = \"myoutput\"\n",
    "    state = 8\n",
    "    n_iter = 10  # 每个模型的参数搜索次数\n",
    "    early_stop_rounds = 50\n",
    "\n",
    "class future_engineer:\n",
    "    traindata = pd.read_csv(cfg.trainfilepath)\n",
    "    testdata = pd.read_csv(cfg.testfilepath)\n",
    "    headerstrian = set(traindata.columns)\n",
    "    headerstest = set(testdata.columns)\n",
    "    target = headerstrian.symmetric_difference(headerstest).pop()\n",
    "    print(\"获取的目标列是：\", target)\n",
    "\n",
    "class dataset_split:\n",
    "    # 划分训练集和验证集（保持原始测试集不变）\n",
    "    X_full = future_engineer.traindata.drop([future_engineer.target], axis=1)\n",
    "    Y_full = future_engineer.traindata[future_engineer.target]\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    X_test = future_engineer.testdata\n",
    "    print(\"数据分隔完成（含验证集）\")\n",
    "\n",
    "class HyperparameterSearch:\n",
    "    @staticmethod\n",
    "    def lgbm_search():\n",
    "        param_dist = {\n",
    "            'learning_rate': [0.005, 0.006,0.007,0.009,0.008,0.01, 0.02],\n",
    "            'max_depth': [4,5,6,7,8],\n",
    "            'colsample_bytree': [0.8, 0.9, 0.95],\n",
    "            'reg_alpha': [0.001, 0.01, 0.1],\n",
    "            'reg_lambda': [0.001, 0.01, 0.1]\n",
    "        }\n",
    "        best_score = np.inf\n",
    "        best_model = None\n",
    "        \n",
    "        for params in ParameterSampler(param_dist, n_iter=cfg.n_iter, random_state=cfg.state):\n",
    "            score_list = []\n",
    "            model = LGBMR(\n",
    "                objective=\"regression\",\n",
    "                n_estimators=10000,\n",
    "                **params,\n",
    "                random_state=cfg.state,\n",
    "                verbosity=-1\n",
    "            )\n",
    "            for train_index, val_index in dataset_split.kf.split(dataset_split.X_full):\n",
    "                # 修改前（错误）\n",
    "                \n",
    "                X_train = dataset_split.X_full.iloc[train_index]\n",
    "                X_val = dataset_split.X_full.iloc[val_index]\n",
    "                y_train = dataset_split.Y_full.iloc[train_index]\n",
    "                y_val = dataset_split.Y_full.iloc[val_index]\n",
    "                model.fit(\n",
    "                    X_train, y_train,\n",
    "                    eval_set=[(X_val, y_val)],\n",
    "                    callbacks=[early_stopping(cfg.early_stop_rounds)],)\n",
    "                pred = model.predict(X_val)\n",
    "                score_list.append(mean_absolute_error(y_val, pred))\n",
    "            score = np.mean(score_list)\n",
    "            score_list = []  # 清空列表以便下次使用\n",
    "            if score < best_score:\n",
    "                best_score = score\n",
    "                best_model = model\n",
    "        print(f\"LGBM 最佳验证MAE: {best_score:.4f}\")\n",
    "        print(best_model)\n",
    "        return best_model\n",
    "\n",
    "    @staticmethod\n",
    "    def xgb_search():\n",
    "        param_dist = {\n",
    "            'learning_rate': [0.005, 0.006,0.007,0.009,0.008,0.01, 0.02],\n",
    "            'max_depth': [4, 5,6, 7,8],\n",
    "            'colsample_bytree': [0.8, 0.9, 0.95],\n",
    "            'reg_alpha': [0.001, 0.01, 0.1],\n",
    "            'gamma': [0, 0.1, 0.2]\n",
    "        }\n",
    "        best_score = np.inf\n",
    "        best_model = None\n",
    "\n",
    "        for params in ParameterSampler(param_dist, n_iter=cfg.n_iter, random_state=cfg.state):\n",
    "            score_list = []\n",
    "            model = XGBR(\n",
    "                objective=\"reg:squarederror\",\n",
    "                n_estimators=10000,\n",
    "                \n",
    "                **params,\n",
    "                random_state=cfg.state,\n",
    "                n_jobs=4,\n",
    "                tree_method=\"gpu_hist\",\n",
    "                predictor=\"gpu_predictor\",\n",
    "                gpu_id=0,\n",
    "                eval_metric=\"mae\",\n",
    "            )\n",
    "            for train_index, val_index in dataset_split.kf.split(dataset_split.X_full):\n",
    "                # 修改前（错误）\n",
    "                \n",
    "                X_train = dataset_split.X_full.iloc[train_index]\n",
    "                X_val = dataset_split.X_full.iloc[val_index]\n",
    "                y_train = dataset_split.Y_full.iloc[train_index]\n",
    "                y_val = dataset_split.Y_full.iloc[val_index]\n",
    "                model.fit(\n",
    "                    X_train, y_train,\n",
    "                    eval_set=[(X_val, y_val)],\n",
    "                    verbose=False,)\n",
    "                pred = model.predict(X_val)\n",
    "                score_list.append(mean_absolute_error(y_val, pred))\n",
    "            score = np.mean(score_list)\n",
    "            score_list = []  # 清空列表以便下次使用\n",
    "            if score < best_score:\n",
    "                best_score = score\n",
    "                best_model = model\n",
    "        print(f\"XGB 最佳验证MAE: {best_score:.4f}\")\n",
    "        print(best_model)\n",
    "        return best_model\n",
    "\n",
    "    @staticmethod\n",
    "    def cat_search():\n",
    "        param_dist = {\n",
    "                'learning_rate': [0.005, 0.006,0.007,0.009,0.008,0.01, 0.02],\n",
    "                'depth': [4,5, 6,7, 8],  # 替换max_depth → depth[3,5,7](@ref)\n",
    "                'rsm': [0.8, 0.9, 0.95],  # 替换colsample_bytree → max_features[3,5](@ref)\n",
    "                'l2_leaf_reg': [0.1, 0.5, 1.0, 10]  # 扩展范围[3](@ref)\n",
    "}\n",
    "        best_score = np.inf\n",
    "        best_model = None\n",
    "\n",
    "        for params in ParameterSampler(param_dist, n_iter=cfg.n_iter, random_state=cfg.state):\n",
    "            score_list = []\n",
    "            model = CBR(\n",
    "                loss_function=\"RMSE\",\n",
    "                iterations=10000,\n",
    "                **params,\n",
    "                random_state=cfg.state,\n",
    "                verbose=0,\n",
    "                early_stopping_rounds=cfg.early_stop_rounds,\n",
    "                task_type=\"GPU\",\n",
    "                devices='0'\n",
    "            )\n",
    "            for train_index, val_index in dataset_split.kf.split(dataset_split.X_full):\n",
    "                # 修改前（错误）\n",
    "                \n",
    "                X_train = dataset_split.X_full.iloc[train_index]\n",
    "                X_val = dataset_split.X_full.iloc[val_index]\n",
    "                y_train = dataset_split.Y_full.iloc[train_index]\n",
    "                y_val = dataset_split.Y_full.iloc[val_index]\n",
    "                model.fit(\n",
    "                    X_train, y_train,\n",
    "                    eval_set=[(X_val, y_val)],\n",
    "                    verbose=False,)\n",
    "                pred = model.predict(X_val)\n",
    "                score_list.append(mean_absolute_error(y_val, pred))\n",
    "            score = np.mean(score_list)\n",
    "            score_list = []  # 清空列表以便下次使用\n",
    "            if score < best_score:\n",
    "                best_score = score\n",
    "                best_model = model\n",
    "        print(f\"CatBoost 最佳验证MAE: {best_score:.4f}\")\n",
    "        print(best_model)\n",
    "        return best_model\n",
    "\n",
    "class model:\n",
    "    models = {\n",
    "        \"LGBM\": HyperparameterSearch.lgbm_search(),\n",
    "        \"XGB\": HyperparameterSearch.xgb_search(),\n",
    "        \"CatBoost\": HyperparameterSearch.cat_search()\n",
    "    }\n",
    "    print(\"\\n所有模型训练完成\")\n",
    "\n",
    "class predict:\n",
    "    predictions = {}\n",
    "    for name, model in model.models.items():\n",
    "        predictions[name] = model.predict(dataset_split.X_test)\n",
    "        print(f\"{name} 预测完成\")\n",
    "\n",
    "class writefile:\n",
    "    test_ids = future_engineer.testdata[\"id\"]\n",
    "    for name, pred in predict.predictions.items():\n",
    "        filename = os.path.join(cfg.outfilepath, f\"{name}_submission.csv\")\n",
    "        pd.DataFrame({\n",
    "            \"id\": test_ids,\n",
    "            future_engineer.target: pred\n",
    "        }).to_csv(filename, index=False)\n",
    "        print(f\"结果已保存至：{filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "293cd468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集 RMSE: 2.1138\n"
     ]
    }
   ],
   "source": [
    "import pandas as  pd\n",
    "from catboost import CatBoostRegressor as CBR\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "kf=KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "df1=pd.read_csv('train_processed.csv')\n",
    "X, y =df1.drop(['Calories'], axis=1).values,df1['Calories'].values\n",
    "model=CBR(\n",
    "    iterations=10000,\n",
    "    depth=6,\n",
    "    learning_rate=0.005,\n",
    "    l2_leaf_reg=1,\n",
    "    loss_function=\"RMSE\",\n",
    "    eval_metric=\"RMSE\",\n",
    "    task_type=\"GPU\",\n",
    "    devices='0',\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X, y, eval_set=(X, y), verbose=0)\n",
    "pred = model.predict(X)\n",
    "mae = mean_absolute_error(y, pred)\n",
    "print(f\"训练集 RMSE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a02d972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "获取的目标列是： Calories\n",
      "数据分隔完成（含验证集）\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's l2: 13.4421\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's l2: 14.004\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's l2: 13.4613\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's l2: 13.1143\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's l2: 13.508\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's l2: 13.3717\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's l2: 13.9918\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's l2: 13.5291\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's l2: 13.1337\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's l2: 13.4989\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's l2: 13.4288\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's l2: 13.9814\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's l2: 13.5139\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's l2: 13.1824\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's l2: 13.5459\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's l2: 14.3727\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's l2: 14.9986\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's l2: 14.5497\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's l2: 14.0924\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's l2: 14.4907\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[999]\tvalid_0's l2: 13.4997\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's l2: 13.9673\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's l2: 13.5798\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's l2: 13.2468\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's l2: 13.7015\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's l2: 13.7486\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's l2: 14.4175\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's l2: 13.8868\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's l2: 13.5653\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's l2: 13.9085\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's l2: 13.8468\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's l2: 14.4254\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's l2: 13.942\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's l2: 13.5795\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's l2: 13.9112\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's l2: 13.5773\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's l2: 14.1999\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's l2: 13.6908\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's l2: 13.386\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's l2: 13.6806\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's l2: 13.3141\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's l2: 13.8795\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's l2: 13.4135\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's l2: 13.0794\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's l2: 13.4805\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's l2: 13.7757\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's l2: 14.3889\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's l2: 13.9088\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's l2: 13.5717\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's l2: 13.874\n",
      "LGBM 最佳验证MAE: 2.2385\n",
      "LGBMRegressor(colsample_bytree=0.95, learning_rate=0.02, max_depth=6,\n",
      "              n_estimators=1000, objective='regression', random_state=8,\n",
      "              reg_alpha=0.1, reg_lambda=0.01, verbosity=-1)\n",
      "XGB 最佳验证MAE: 2.1484\n",
      "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=0.95, device=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric='mae', feature_types=None,\n",
      "             feature_weights=None, gamma=0, gpu_id=0, grow_policy=None,\n",
      "             importance_type=None, interaction_constraints=None,\n",
      "             learning_rate=0.016, max_bin=None, max_cat_threshold=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=8,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, multi_strategy=None, n_estimators=1000,\n",
      "             n_jobs=4, ...)\n",
      "CatBoost 最佳验证MAE: 2.1404\n",
      "<catboost.core.CatBoostRegressor object at 0x000002018FE09C60>\n",
      "\n",
      "所有基础模型训练完成\n",
      "LGBM 预测完成\n",
      "XGB 预测完成\n",
      "CatBoost 预测完成\n",
      "\n",
      "开始生成第一层Stacking特征...\n",
      "生成 LGBM 的交叉验证预测...\n",
      "生成 XGB 的交叉验证预测...\n",
      "生成 CatBoost 的交叉验证预测...\n",
      "使用 LGBM 生成测试集预测...\n",
      "使用 XGB 生成测试集预测...\n",
      "使用 CatBoost 生成测试集预测...\n",
      "第一层Stacking特征生成完成\n",
      "\n",
      "开始训练第二层模型...\n",
      "训练第二层模型 Ridge...\n",
      "训练第二层模型 Lasso...\n",
      "训练第二层模型 ElasticNet...\n",
      "训练第二层模型 LGBM_meta...\n",
      "第二层模型训练完成\n",
      "\n",
      "为测试集生成第二层特征...\n",
      "使用第二层模型 Ridge 生成测试集预测...\n",
      "使用第二层模型 Lasso 生成测试集预测...\n",
      "使用第二层模型 ElasticNet 生成测试集预测...\n",
      "使用第二层模型 LGBM_meta 生成测试集预测...\n",
      "第二层测试集特征生成完成\n",
      "\n",
      "训练最终模型...\n",
      "最终模型交叉验证MAE: 2.1183 ± 0.0037\n",
      "最终模型训练完成\n",
      "\n",
      "两层Stacking模型训练完成，预测结果已保存\n",
      "Stacking预测完成\n",
      "结果已保存至：myoutput\\LGBM_submission.csv\n",
      "结果已保存至：myoutput\\XGB_submission.csv\n",
      "结果已保存至：myoutput\\CatBoost_submission.csv\n",
      "结果已保存至：myoutput\\Stacking_submission.csv\n",
      "\n",
      "各模型预测结果比较:\n",
      "LGBM vs Stacking MAE差异: 0.6343\n",
      "XGB vs Stacking MAE差异: 0.3464\n",
      "CatBoost vs Stacking MAE差异: 0.3060\n",
      "\n",
      "模型预测相关性矩阵:\n",
      "              LGBM       XGB  CatBoost  Stacking\n",
      "LGBM      1.000000  0.999840  0.999846  0.999896\n",
      "XGB       0.999840  1.000000  0.999865  0.999960\n",
      "CatBoost  0.999846  0.999865  1.000000  0.999970\n",
      "Stacking  0.999896  0.999960  0.999970  1.000000\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import KFold\n",
    "from lightgbm import LGBMRegressor as LGBMR, early_stopping\n",
    "from xgboost import XGBRegressor as XGBR\n",
    "from catboost import CatBoostRegressor as CBR\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split, ParameterSampler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "class cfg:\n",
    "    trainfilepath = \"train_processed.csv\"\n",
    "    testfilepath = \"test_processed.csv\"\n",
    "    outfilepath = \"myoutput\"\n",
    "    state = 8\n",
    "    n_iter = 10  # 每个模型的参数搜索次数\n",
    "    early_stop_rounds = 50\n",
    "\n",
    "class future_engineer:\n",
    "    traindata = pd.read_csv(cfg.trainfilepath)\n",
    "    testdata = pd.read_csv(cfg.testfilepath)\n",
    "    headerstrian = set(traindata.columns)\n",
    "    headerstest = set(testdata.columns)\n",
    "    target = headerstrian.symmetric_difference(headerstest).pop()\n",
    "    print(\"获取的目标列是：\", target)\n",
    "\n",
    "class dataset_split:\n",
    "    # 划分训练集和验证集（保持原始测试集不变）\n",
    "    X_full = future_engineer.traindata.drop([future_engineer.target], axis=1)\n",
    "    Y_full = future_engineer.traindata[future_engineer.target]\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    X_test = future_engineer.testdata\n",
    "    print(\"数据分隔完成（含验证集）\")\n",
    "\n",
    "class HyperparameterSearch:\n",
    "    @staticmethod\n",
    "    def lgbm_search():\n",
    "        param_dist = {\n",
    "            'learning_rate': [0.012, 0.016,0.014,0.018,0.01, 0.02],\n",
    "            'max_depth': [4,5,6,7,8],\n",
    "            'colsample_bytree': [0.8, 0.9, 0.95],\n",
    "            'reg_alpha': [0.001, 0.01, 0.1],\n",
    "            'reg_lambda': [0.001, 0.01, 0.1]\n",
    "        }\n",
    "        best_score = np.inf\n",
    "        best_model = None\n",
    "        \n",
    "        for params in ParameterSampler(param_dist, n_iter=cfg.n_iter, random_state=cfg.state):\n",
    "            score_list = []\n",
    "            model = LGBMR(\n",
    "                objective=\"regression\",\n",
    "                n_estimators=1000,\n",
    "                **params,\n",
    "                random_state=cfg.state,\n",
    "                verbosity=-1\n",
    "            )\n",
    "            for train_index, val_index in dataset_split.kf.split(dataset_split.X_full):\n",
    "                # 修改前（错误）\n",
    "                \n",
    "                X_train = dataset_split.X_full.iloc[train_index]\n",
    "                X_val = dataset_split.X_full.iloc[val_index]\n",
    "                y_train = dataset_split.Y_full.iloc[train_index]\n",
    "                y_val = dataset_split.Y_full.iloc[val_index]\n",
    "                model.fit(\n",
    "                    X_train, y_train,\n",
    "                    eval_set=[(X_val, y_val)],\n",
    "                    callbacks=[early_stopping(cfg.early_stop_rounds)],)\n",
    "                pred = model.predict(X_val)\n",
    "                score_list.append(mean_absolute_error(y_val, pred))\n",
    "            score = np.mean(score_list)\n",
    "            score_list = []  # 清空列表以便下次使用\n",
    "            if score < best_score:\n",
    "                best_score = score\n",
    "                best_model = model\n",
    "        print(f\"LGBM 最佳验证MAE: {best_score:.4f}\")\n",
    "        print(best_model)\n",
    "        return best_model\n",
    "\n",
    "    @staticmethod\n",
    "    def xgb_search():\n",
    "        param_dist = {\n",
    "            'learning_rate': [0.012, 0.016,0.014,0.018,0.01, 0.02],\n",
    "            'max_depth': [4, 5,6, 7,8],\n",
    "            'colsample_bytree': [0.8, 0.9, 0.95],\n",
    "            'reg_alpha': [0.001, 0.01, 0.1],\n",
    "            'gamma': [0, 0.1, 0.2]\n",
    "        }\n",
    "        best_score = np.inf\n",
    "        best_model = None\n",
    "\n",
    "        for params in ParameterSampler(param_dist, n_iter=cfg.n_iter, random_state=cfg.state):\n",
    "            score_list = []\n",
    "            model = XGBR(\n",
    "                objective=\"reg:squarederror\",\n",
    "                n_estimators=1000,\n",
    "                \n",
    "                **params,\n",
    "                random_state=cfg.state,\n",
    "                n_jobs=4,\n",
    "                tree_method=\"gpu_hist\",\n",
    "                predictor=\"gpu_predictor\",\n",
    "                gpu_id=0,\n",
    "                eval_metric=\"mae\",\n",
    "            )\n",
    "            for train_index, val_index in dataset_split.kf.split(dataset_split.X_full):\n",
    "                # 修改前（错误）\n",
    "                \n",
    "                X_train = dataset_split.X_full.iloc[train_index]\n",
    "                X_val = dataset_split.X_full.iloc[val_index]\n",
    "                y_train = dataset_split.Y_full.iloc[train_index]\n",
    "                y_val = dataset_split.Y_full.iloc[val_index]\n",
    "                model.fit(\n",
    "                    X_train, y_train,\n",
    "                    eval_set=[(X_val, y_val)],\n",
    "                    verbose=False,)\n",
    "                pred = model.predict(X_val)\n",
    "                score_list.append(mean_absolute_error(y_val, pred))\n",
    "            score = np.mean(score_list)\n",
    "            score_list = []  # 清空列表以便下次使用\n",
    "            if score < best_score:\n",
    "                best_score = score\n",
    "                best_model = model\n",
    "        print(f\"XGB 最佳验证MAE: {best_score:.4f}\")\n",
    "        print(best_model)\n",
    "        return best_model\n",
    "\n",
    "    @staticmethod\n",
    "    def cat_search():\n",
    "        param_dist = {\n",
    "                'learning_rate': [0.012, 0.016,0.014,0.018,0.01, 0.02],\n",
    "                'depth': [4,5, 6,7, 8],  # 替换max_depth → depth[3,5,7](@ref)\n",
    "                \n",
    "                'l2_leaf_reg': [0.1, 0.5, 1.0, 10]  # 扩展范围[3](@ref)\n",
    "}\n",
    "        best_score = np.inf\n",
    "        best_model = None\n",
    "\n",
    "        for params in ParameterSampler(param_dist, n_iter=cfg.n_iter, random_state=cfg.state):\n",
    "            score_list = []\n",
    "            model = CBR(\n",
    "                loss_function=\"RMSE\",\n",
    "                iterations=1000,\n",
    "                **params,\n",
    "                random_state=cfg.state,\n",
    "                verbose=0,\n",
    "                early_stopping_rounds=cfg.early_stop_rounds,\n",
    "                task_type=\"GPU\",\n",
    "                devices='0'\n",
    "            )\n",
    "            for train_index, val_index in dataset_split.kf.split(dataset_split.X_full):\n",
    "                # 修改前（错误）\n",
    "                \n",
    "                X_train = dataset_split.X_full.iloc[train_index]\n",
    "                X_val = dataset_split.X_full.iloc[val_index]\n",
    "                y_train = dataset_split.Y_full.iloc[train_index]\n",
    "                y_val = dataset_split.Y_full.iloc[val_index]\n",
    "                model.fit(\n",
    "                    X_train, y_train,\n",
    "                    eval_set=[(X_val, y_val)],\n",
    "                    verbose=False,)\n",
    "                pred = model.predict(X_val)\n",
    "                score_list.append(mean_absolute_error(y_val, pred))\n",
    "            score = np.mean(score_list)\n",
    "            score_list = []  # 清空列表以便下次使用\n",
    "            if score < best_score:\n",
    "                best_score = score\n",
    "                best_model = model\n",
    "        print(f\"CatBoost 最佳验证MAE: {best_score:.4f}\")\n",
    "        print(best_model)\n",
    "        return best_model\n",
    "\n",
    "\n",
    "class model:\n",
    "    models = {\n",
    "        \"LGBM\": HyperparameterSearch.lgbm_search(),\n",
    "        \"XGB\": HyperparameterSearch.xgb_search(),\n",
    "        \"CatBoost\": HyperparameterSearch.cat_search()\n",
    "    }\n",
    "    print(\"\\n所有基础模型训练完成\")\n",
    "\n",
    "class StackingModel:\n",
    "    @staticmethod\n",
    "    def generate_first_level_features():\n",
    "        \"\"\"生成第一层模型的特征（基于交叉验证）\"\"\"\n",
    "        print(\"\\n开始生成第一层Stacking特征...\")\n",
    "        X = dataset_split.X_full\n",
    "        y = dataset_split.Y_full\n",
    "        \n",
    "        # 为训练集创建空的特征矩阵\n",
    "        first_level_train = np.zeros((X.shape[0], len(model.models)))\n",
    "        \n",
    "        # 使用交叉验证生成训练集特征\n",
    "        for i, (name, m) in enumerate(model.models.items()):\n",
    "            print(f\"生成 {name} 的交叉验证预测...\")\n",
    "            for train_idx, val_idx in dataset_split.kf.split(X):\n",
    "                X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "                y_train = y.iloc[train_idx]\n",
    "                \n",
    "                # 克隆模型并在训练集上训练\n",
    "                if name == \"LGBM\":\n",
    "                    clone_model = LGBMR(**m.get_params())\n",
    "                elif name == \"XGB\":\n",
    "                    clone_model = XGBR(**m.get_params())\n",
    "                elif name == \"CatBoost\":\n",
    "                    clone_model = CBR(**m.get_params())\n",
    "                \n",
    "                clone_model.fit(X_train, y_train)\n",
    "                first_level_train[val_idx, i] = clone_model.predict(X_val)\n",
    "        \n",
    "        # 为测试集创建特征矩阵\n",
    "        first_level_test = np.zeros((dataset_split.X_test.shape[0], len(model.models)))\n",
    "        for i, (name, m) in enumerate(model.models.items()):\n",
    "            print(f\"使用 {name} 生成测试集预测...\")\n",
    "            first_level_test[:, i] = m.predict(dataset_split.X_test)\n",
    "        \n",
    "        print(\"第一层Stacking特征生成完成\")\n",
    "        return first_level_train, first_level_test\n",
    "    \n",
    "    @staticmethod\n",
    "    def train_second_level_models(first_level_train, y):\n",
    "        \"\"\"训练第二层模型\"\"\"\n",
    "        print(\"\\n开始训练第二层模型...\")\n",
    "        second_level_models = {\n",
    "            \"Ridge\": Ridge(alpha=1.0, random_state=cfg.state),\n",
    "            \"Lasso\": Lasso(alpha=0.01, random_state=cfg.state),\n",
    "            \"ElasticNet\": ElasticNet(alpha=0.01, l1_ratio=0.5, random_state=cfg.state),\n",
    "            \"LGBM_meta\": LGBMR(n_estimators=200, learning_rate=0.01, random_state=cfg.state)\n",
    "        }\n",
    "        \n",
    "        # 为训练集创建空的第二层特征矩阵\n",
    "        second_level_train = np.zeros((first_level_train.shape[0], len(second_level_models)))\n",
    "        \n",
    "        # 使用交叉验证生成第二层训练集特征\n",
    "        for i, (name, m) in enumerate(second_level_models.items()):\n",
    "            print(f\"训练第二层模型 {name}...\")\n",
    "            for train_idx, val_idx in dataset_split.kf.split(first_level_train):\n",
    "                X_train, X_val = first_level_train[train_idx], first_level_train[val_idx]\n",
    "                y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "                \n",
    "                # 克隆模型并训练\n",
    "                if name == \"LGBM_meta\":\n",
    "                    clone_model = LGBMR(**m.get_params())\n",
    "                elif name == \"Ridge\":\n",
    "                    clone_model = Ridge(**m.get_params())\n",
    "                elif name == \"Lasso\":\n",
    "                    clone_model = Lasso(**m.get_params())\n",
    "                elif name == \"ElasticNet\":\n",
    "                    clone_model = ElasticNet(**m.get_params())\n",
    "                \n",
    "                clone_model.fit(X_train, y_train)\n",
    "                second_level_train[val_idx, i] = clone_model.predict(X_val)\n",
    "            \n",
    "            # 在全部数据上重新训练模型\n",
    "            m.fit(first_level_train, y)\n",
    "        \n",
    "        print(\"第二层模型训练完成\")\n",
    "        return second_level_models, second_level_train\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_second_level_test_features(first_level_test, second_level_models):\n",
    "        \"\"\"为测试集生成第二层特征\"\"\"\n",
    "        print(\"\\n为测试集生成第二层特征...\")\n",
    "        second_level_test = np.zeros((first_level_test.shape[0], len(second_level_models)))\n",
    "        \n",
    "        for i, (name, m) in enumerate(second_level_models.items()):\n",
    "            print(f\"使用第二层模型 {name} 生成测试集预测...\")\n",
    "            second_level_test[:, i] = m.predict(first_level_test)\n",
    "        \n",
    "        print(\"第二层测试集特征生成完成\")\n",
    "        return second_level_test\n",
    "    \n",
    "    @staticmethod\n",
    "    def train_final_model(second_level_train, y):\n",
    "        \"\"\"训练最终模型\"\"\"\n",
    "        print(\"\\n训练最终模型...\")\n",
    "        final_model = Ridge(alpha=0.5, random_state=cfg.state)\n",
    "        \n",
    "        # 评估最终模型性能\n",
    "        final_scores = []\n",
    "        for train_idx, val_idx in dataset_split.kf.split(second_level_train):\n",
    "            X_train, X_val = second_level_train[train_idx], second_level_train[val_idx]\n",
    "            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "            \n",
    "            final_model.fit(X_train, y_train)\n",
    "            pred = final_model.predict(X_val)\n",
    "            score = mean_absolute_error(y_val, pred)\n",
    "            final_scores.append(score)\n",
    "        \n",
    "        print(f\"最终模型交叉验证MAE: {np.mean(final_scores):.4f} ± {np.std(final_scores):.4f}\")\n",
    "        \n",
    "        # 在全部数据上重新训练最终模型\n",
    "        final_model.fit(second_level_train, y)\n",
    "        print(\"最终模型训练完成\")\n",
    "        return final_model\n",
    "    \n",
    "    @staticmethod\n",
    "    def run_stacking():\n",
    "        \"\"\"执行完整的两层Stacking流程\"\"\"\n",
    "        # 生成第一层特征\n",
    "        first_level_train, first_level_test = StackingModel.generate_first_level_features()\n",
    "        \n",
    "        # 训练第二层模型\n",
    "        second_level_models, second_level_train = StackingModel.train_second_level_models(\n",
    "            first_level_train, dataset_split.Y_full)\n",
    "        \n",
    "        # 为测试集生成第二层特征\n",
    "        second_level_test = StackingModel.generate_second_level_test_features(\n",
    "            first_level_test, second_level_models)\n",
    "        \n",
    "        # 训练最终模型\n",
    "        final_model = StackingModel.train_final_model(second_level_train, dataset_split.Y_full)\n",
    "        \n",
    "        # 生成最终预测\n",
    "        final_prediction = final_model.predict(second_level_test)\n",
    "        \n",
    "        # 保存模型和预测结果\n",
    "        stacking_models = {\n",
    "            'base_models': model.models,\n",
    "            'second_level_models': second_level_models,\n",
    "            'final_model': final_model\n",
    "        }\n",
    "        \n",
    "        # 创建输出目录（如果不存在）\n",
    "        if not os.path.exists(cfg.outfilepath):\n",
    "            os.makedirs(cfg.outfilepath)\n",
    "        \n",
    "        # 保存模型\n",
    "        with open(os.path.join(cfg.outfilepath, 'stacking_models.pkl'), 'wb') as f:\n",
    "            pickle.dump(stacking_models, f)\n",
    "        \n",
    "        # 保存预测结果\n",
    "        test_ids = future_engineer.testdata[\"id\"]\n",
    "        pd.DataFrame({\n",
    "            \"id\": test_ids,\n",
    "            future_engineer.target: final_prediction\n",
    "        }).to_csv(os.path.join(cfg.outfilepath, \"stacking_submission.csv\"), index=False)\n",
    "        \n",
    "        print(\"\\n两层Stacking模型训练完成，预测结果已保存\")\n",
    "        return final_prediction\n",
    "\n",
    "class predict:\n",
    "    # 运行单个模型预测\n",
    "    predictions = {}\n",
    "    for name, model in model.models.items():\n",
    "        predictions[name] = model.predict(dataset_split.X_test)\n",
    "        print(f\"{name} 预测完成\")\n",
    "    \n",
    "    # 运行Stacking模型\n",
    "    stacking_prediction = StackingModel.run_stacking()\n",
    "    predictions[\"Stacking\"] = stacking_prediction\n",
    "    print(\"Stacking预测完成\")\n",
    "\n",
    "class writefile:\n",
    "    test_ids = future_engineer.testdata[\"id\"]\n",
    "    for name, pred in predict.predictions.items():\n",
    "        filename = os.path.join(cfg.outfilepath, f\"{name}_submission.csv\")\n",
    "        pd.DataFrame({\n",
    "            \"id\": test_ids,\n",
    "            future_engineer.target: pred\n",
    "        }).to_csv(filename, index=False)\n",
    "        print(f\"结果已保存至：{filename}\")\n",
    "\n",
    "# 比较各模型性能（可选）\n",
    "def compare_models():\n",
    "    print(\"\\n各模型预测结果比较:\")\n",
    "    for name, pred in predict.predictions.items():\n",
    "        if name != \"Stacking\":  # 基础模型\n",
    "            base_mae = mean_absolute_error(predict.predictions[\"Stacking\"], pred)\n",
    "            print(f\"{name} vs Stacking MAE差异: {base_mae:.4f}\")\n",
    "    \n",
    "    # 计算各模型预测的相关性\n",
    "    pred_df = pd.DataFrame(predict.predictions)\n",
    "    correlation = pred_df.corr()\n",
    "    print(\"\\n模型预测相关性矩阵:\")\n",
    "    print(correlation)\n",
    "\n",
    "# 执行模型比较\n",
    "compare_models()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JUNzi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
